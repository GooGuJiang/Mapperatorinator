"""
Mapperatorinator API - é‡æ„ç‰ˆæœ¬
æ”¯æŒéŸ³é¢‘æ–‡ä»¶å’Œå‚æ•°ä¸€èµ·ä¸Šä¼ ï¼Œå›ºå®šæ–‡ä»¶å¤¹å­˜å‚¨ï¼Œå¤„ç†å®Œæˆåä¸‹è½½ç»“æœ
"""

import asyncio
import json
import os
import subprocess
import sys
import threading
import time
import uuid
import glob
from pathlib import Path
from typing import Dict, List, Optional, Any

try:
    import uvicorn
    from fastapi import FastAPI, File, Form, HTTPException, UploadFile
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.responses import FileResponse
    from pydantic import BaseModel, Field
    from sse_starlette.sse import EventSourceResponse
    import redis
    import redis.exceptions
    # å¯é€‰ï¼šåŠ è½½.envæ–‡ä»¶
    try:
        from dotenv import load_dotenv
        load_dotenv()  # åŠ è½½.envæ–‡ä»¶åˆ°ç¯å¢ƒå˜é‡
        print("ğŸ“„ å·²åŠ è½½.envæ–‡ä»¶")
    except ImportError:
        print("ğŸ’¡ æç¤ºï¼šå®‰è£…python-dotenvå¯è‡ªåŠ¨åŠ è½½.envæ–‡ä»¶: pip install python-dotenv")
except ImportError as e:
    print(f"ç¼ºå°‘å¿…è¦çš„åŒ…ï¼Œè¯·å®‰è£…: pip install fastapi uvicorn sse-starlette redis")
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)

from config import InferenceConfig

# å…¨å±€å˜é‡
active_processes: Dict[str, subprocess.Popen] = {}
process_outputs: Dict[str, List[str]] = {}
job_metadata: Dict[str, Dict] = {}
job_progress: Dict[str, Dict] = {}  # æ–°å¢è¿›åº¦è¿½è¸ª
process_lock = threading.Lock()

# Redisè¿æ¥ - ä½¿ç”¨db1
redis_client = None
try:
    redis_client = redis.Redis(
        host=os.getenv('REDIS_HOST', 'localhost'),
        port=int(os.getenv('REDIS_PORT', 6379)),
        db=1,  # ä½¿ç”¨db1
        decode_responses=True,
        socket_connect_timeout=5,
        socket_timeout=5,
        retry_on_timeout=True
    )
    # æµ‹è¯•è¿æ¥
    redis_client.ping()
    print("âœ… Redisè¿æ¥æˆåŠŸ (db=1)")
except (redis.exceptions.RedisError, ConnectionError, Exception) as e:
    print(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨å†…å­˜ç¼“å­˜: {e}")
    redis_client = None

# å›ºå®šç›®å½•
AUDIO_STORAGE = Path("audio_storage")  # éŸ³é¢‘å­˜å‚¨ç›®å½•
OUTPUTS = Path("outputs")              # è¾“å‡ºç›®å½•
AUDIO_STORAGE.mkdir(exist_ok=True)
OUTPUTS.mkdir(exist_ok=True)

app = FastAPI(
    title="Mapperatorinator API",
    description="AIç”Ÿæˆosu! beatmapçš„APIæ¥å£",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Redisç¼“å­˜è¾…åŠ©å‡½æ•°
def cache_set(key: str, value: Any, expire: int = 3600):
    """è®¾ç½®ç¼“å­˜ï¼Œé»˜è®¤1å°æ—¶è¿‡æœŸ"""
    if redis_client:
        try:
            redis_client.setex(key, expire, json.dumps(value))
            return True
        except redis.exceptions.RedisError as e:
            print(f"Redisè®¾ç½®å¤±è´¥: {e}")
    return False

def cache_get(key: str) -> Optional[Any]:
    """è·å–ç¼“å­˜"""
    if redis_client:
        try:
            data = redis_client.get(key)
            if data and isinstance(data, (str, bytes)):
                return json.loads(data)
            return None
        except (redis.exceptions.RedisError, json.JSONDecodeError) as e:
            print(f"Redisè·å–å¤±è´¥: {e}")
    return None

def cache_delete(key: str):
    """åˆ é™¤ç¼“å­˜"""
    if redis_client:
        try:
            redis_client.delete(key)
            return True
        except redis.exceptions.RedisError as e:
            print(f"Redisåˆ é™¤å¤±è´¥: {e}")
    return False

def cache_exists(key: str) -> bool:
    """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
    if redis_client:
        try:
            exists_result = redis_client.exists(key)
            # å¤„ç†ä¸åŒç±»å‹çš„è¿”å›å€¼
            if isinstance(exists_result, int):
                return exists_result > 0
            else:
                return bool(exists_result)
        except redis.exceptions.RedisError as e:
            print(f"Redisæ£€æŸ¥å¤±è´¥: {e}")
    return False

def cache_job_progress(job_id: str):
    """ç¼“å­˜ä»»åŠ¡è¿›åº¦ä¿¡æ¯"""
    progress_info = job_progress.get(job_id)
    if progress_info:
        cache_set(f"job_progress:{job_id}", progress_info, 7200)  # 2å°æ—¶è¿‡æœŸ

def get_cached_job_progress(job_id: str) -> Optional[Dict]:
    """è·å–ç¼“å­˜çš„ä»»åŠ¡è¿›åº¦"""
    return cache_get(f"job_progress:{job_id}")

def cache_job_metadata(job_id: str):
    """ç¼“å­˜ä»»åŠ¡å…ƒæ•°æ®"""
    metadata = job_metadata.get(job_id)
    if metadata:
        # ç§»é™¤ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡
        serializable_metadata = {k: v for k, v in metadata.items() if k != 'process'}
        cache_set(f"job_metadata:{job_id}", serializable_metadata, 7200)

def get_cached_job_metadata(job_id: str) -> Optional[Dict]:
    """è·å–ç¼“å­˜çš„ä»»åŠ¡å…ƒæ•°æ®"""
    return cache_get(f"job_metadata:{job_id}")

def cache_output_files(job_id: str, files: List[str]):
    """ç¼“å­˜è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"""
    cache_set(f"output_files:{job_id}", files, 3600)  # 1å°æ—¶è¿‡æœŸ

def get_cached_output_files(job_id: str) -> Optional[List[str]]:
    """è·å–ç¼“å­˜çš„è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"""
    return cache_get(f"output_files:{job_id}")

def cache_model_config(config_name: str, config_data: Dict):
    """ç¼“å­˜æ¨¡å‹é…ç½®"""
    cache_set(f"model_config:{config_name}", config_data, 86400)  # 24å°æ—¶è¿‡æœŸ

def get_cached_model_config(config_name: str) -> Optional[Dict]:
    """è·å–ç¼“å­˜çš„æ¨¡å‹é…ç½®"""
    return cache_get(f"model_config:{config_name}")

# å“åº”æ¨¡å‹
class ProcessResponse(BaseModel):
    """å¤„ç†å“åº”æ¨¡å‹"""
    job_id: str = Field(..., description="ä»»åŠ¡ID")
    status: str = Field(..., description="çŠ¶æ€")
    message: str = Field(..., description="æ¶ˆæ¯")

class ProgressResponse(BaseModel):
    """è¿›åº¦å“åº”æ¨¡å‹"""
    job_id: str = Field(..., description="ä»»åŠ¡ID")
    progress: float = Field(..., description="è¿›åº¦ç™¾åˆ†æ¯” (0-100)")
    stage: str = Field(..., description="å½“å‰é˜¶æ®µ")
    estimated: bool = Field(..., description="æ˜¯å¦ä¸ºä¼°ç®—è¿›åº¦")
    last_update: float = Field(..., description="æœ€åæ›´æ–°æ—¶é—´æˆ³")
    status: str = Field(..., description="ä»»åŠ¡çŠ¶æ€")

class JobStatus(BaseModel):
    """ä»»åŠ¡çŠ¶æ€æ¨¡å‹"""
    job_id: str = Field(..., description="ä»»åŠ¡ID")
    status: str = Field(..., description="å½“å‰çŠ¶æ€")
    message: Optional[str] = Field(None, description="çŠ¶æ€æ¶ˆæ¯")
    progress: Optional[float] = Field(None, description="è¿›åº¦ç™¾åˆ†æ¯”")
    output_files: Optional[List[str]] = Field(None, description="è¾“å‡ºæ–‡ä»¶åˆ—è¡¨")
    error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")

def parse_progress_from_output(output_line: str) -> Optional[float]:
    """ä»è¾“å‡ºè¡Œè§£æè¿›åº¦ç™¾åˆ†æ¯” - æ”¯æŒtqdmå’Œå…¶ä»–è¿›åº¦æ ¼å¼"""
    import re
    
    # tqdmè¿›åº¦æ¡æ ¼å¼ï¼šåŒ¹é… "æ•°å­—%|è¿›åº¦æ¡| æ•°å­—/æ€»æ•°" æˆ– "æ•°å­—%|"
    tqdm_patterns = [
        r'^\s*(\d+)%\|.*?\|\s*(\d+)/(\d+)',  # å®Œæ•´tqdm: "  0%|          | 0/65"
        r'^\s*(\d+)%\|',                     # ç®€åŒ–tqdm: "  0%|"
        r'(\d+)%\|.*?\|\s*(\d+)/(\d+)',      # è¡Œä¸­çš„tqdmæ ¼å¼
    ]
    
    for pattern in tqdm_patterns:
        match = re.search(pattern, output_line)
        if match:
            try:
                if len(match.groups()) == 3:
                    # å®Œæ•´æ ¼å¼ï¼Œä½¿ç”¨åˆ†æ•°è®¡ç®—æ›´ç²¾ç¡®çš„è¿›åº¦
                    percent_display = float(match.group(1))
                    current = float(match.group(2))
                    total = float(match.group(3))
                    if total > 0:
                        actual_percent = (current / total) * 100
                        # ä½¿ç”¨æ›´ç²¾ç¡®çš„åˆ†æ•°è®¡ç®—ç»“æœ
                        return min(100.0, max(0.0, actual_percent))
                    else:
                        return min(100.0, max(0.0, percent_display))
                else:
                    # ç®€åŒ–æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨ç™¾åˆ†æ¯”
                    percent = float(match.group(1))
                    return min(100.0, max(0.0, percent))
            except ValueError:
                continue
    
    # å¤‡ç”¨æ¨¡å¼ï¼šå…¶ä»–å¸¸è§è¿›åº¦æ ¼å¼
    backup_patterns = [
        r'(\d+)%(?!\|)',                    # ç®€å•ç™¾åˆ†æ¯”: 50% (ä½†ä¸æ˜¯ 50%|)
        r'(\d+)/(\d+)',                     # åˆ†æ•°æ ¼å¼: 50/100
        r'Progress:\s*(\d+(?:\.\d+)?)%',    # Progress: 50.5%
        r'(\d+(?:\.\d+)?)%\s*complete',     # 50.5% complete
        r'Step\s+(\d+)\s+of\s+(\d+)',       # Step 5 of 10
        r'Processing.*?(\d+)%',             # Processing... 50%
        r'Generating.*?(\d+)%',             # Generating... 50%
    ]
    
    for pattern in backup_patterns:
        match = re.search(pattern, output_line, re.IGNORECASE)
        if match:
            try:
                if len(match.groups()) == 1:
                    # ç›´æ¥ç™¾åˆ†æ¯”
                    percent = float(match.group(1))
                    return min(100.0, max(0.0, percent))
                elif len(match.groups()) == 2:
                    # åˆ†æ•°æ ¼å¼ï¼Œè®¡ç®—ç™¾åˆ†æ¯”
                    current = float(match.group(1))
                    total = float(match.group(2))
                    if total > 0:
                        percent = (current / total) * 100
                        return min(100.0, max(0.0, percent))
            except ValueError:
                continue
    
    return None

def estimate_progress_from_stage(output_line: str, current_progress: float) -> Optional[Dict[str, Any]]:
    """æ ¹æ®å¤„ç†é˜¶æ®µä¼°ç®—è¿›åº¦ - å‚è€ƒweb-ui.jsçš„é˜¶æ®µè¯†åˆ«"""
    
    # åŸºäºå®é™…inference.pyè¾“å‡ºçš„å…³é”®è¯
    stage_keywords = {
        # å®é™…è§‚å¯Ÿåˆ°çš„å…³é”®è¯ï¼ˆä»ç”¨æˆ·æä¾›çš„è¾“å‡ºï¼‰
        "using cuda for inference": ("initializing", 0, 5),
        "using mps for inference": ("initializing", 0, 5),
        "using cpu for inference": ("initializing", 0, 5),
        "random seed": ("loading_model", 5, 10),
        "model loaded": ("model_ready", 10, 15),
        "generating map": ("generating_map", 15, 85),
        "generating timing": ("generating_timing", 15, 40),
        "generating kiai": ("generating_kiai", 40, 60),
        "generated beatmap saved": ("saving", 85, 95),
        "generated .osz saved": ("completed", 95, 100),
        
        # web-ui.jsä¸­çš„progressTitleså¯¹åº”å…³é”®è¯
        "seq len": ("refining_positions", 85, 95),
        
        # å…¶ä»–å¯èƒ½çš„å…³é”®è¯
        "loading": ("loading", 0, 10),
        "load": ("loading", 0, 10),
        "initializing": ("initializing", 0, 5),
        "preprocessing": ("preprocessing", 5, 15),
        "processing": ("processing", 10, 50),
        "inference": ("inference", 30, 80),
        "generating": ("generating", 40, 85),
        "postprocessing": ("postprocessing", 85, 95),
        "saving": ("saving", 95, 100),
        "export": ("export", 95, 100),
        "complete": ("completed", 100, 100),
        "finished": ("completed", 100, 100),
        "done": ("completed", 100, 100),
        
        # æ¨¡å‹ç›¸å…³å…³é”®è¯
        "model": ("loading", 0, 10),
        "tokenizer": ("loading", 5, 15),
        "config": ("loading", 0, 10),
        "checkpoint": ("loading", 5, 15),
        
        # éŸ³é¢‘å¤„ç†å…³é”®è¯
        "audio": ("preprocessing", 10, 25),
        "spectrogram": ("preprocessing", 15, 30),
        "feature": ("preprocessing", 20, 35),
        
        # CUDA/è®¾å¤‡å…³é”®è¯
        "cuda": ("initializing", 0, 5),
        "device": ("initializing", 0, 5),
        "gpu": ("initializing", 0, 5),
        
        # é”™è¯¯å…³é”®è¯
        "error": ("error", current_progress, current_progress),
        "failed": ("error", current_progress, current_progress),
        "exception": ("error", current_progress, current_progress),
        "traceback": ("error", current_progress, current_progress),
    }
    
    line_lower = output_line.lower()
    
    # æŸ¥æ‰¾æœ€ä½³åŒ¹é…çš„å…³é”®è¯
    best_match = None
    best_keyword_len = 0
    
    for keyword, (stage_name, start, end) in stage_keywords.items():
        if keyword in line_lower:
            # ä¼˜å…ˆé€‰æ‹©æ›´é•¿çš„å…³é”®è¯åŒ¹é…ï¼ˆæ›´å…·ä½“ï¼‰
            if len(keyword) > best_keyword_len:
                best_match = (stage_name, start, end)
                best_keyword_len = len(keyword)
    
    if best_match:
        stage_name, start, end = best_match
        # å¦‚æœæ£€æµ‹åˆ°æ–°é˜¶æ®µï¼Œæ›´æ–°è¿›åº¦åˆ°è¯¥é˜¶æ®µçš„å¼€å§‹ç‚¹
        if current_progress < start:
            return {
                "progress": float(start),
                "stage": stage_name,
                "estimated": True
            }
        # å¦‚æœåœ¨é˜¶æ®µèŒƒå›´å†…ï¼Œä¿æŒå½“å‰è¿›åº¦ä½†æ›´æ–°é˜¶æ®µå
        elif start <= current_progress <= end:
            return {
                "progress": current_progress,
                "stage": stage_name,
                "estimated": True
            }
        # å¦‚æœè¿›åº¦å·²è¶…è¿‡è¯¥é˜¶æ®µï¼Œç»§ç»­ä½¿ç”¨å½“å‰è¿›åº¦
        else:
            return {
                "progress": current_progress,
                "stage": stage_name,
                "estimated": True
            }
    
    return None

def update_job_progress(job_id: str, output_line: str):
    """æ›´æ–°ä»»åŠ¡è¿›åº¦ - å‚è€ƒweb-ui.pyçš„è¿›åº¦è§£æé€»è¾‘ï¼Œæ”¯æŒRedisç¼“å­˜"""
    with process_lock:
        if job_id not in job_progress:
            # å°è¯•ä»ç¼“å­˜åŠ è½½è¿›åº¦ä¿¡æ¯
            cached_progress = get_cached_job_progress(job_id)
            if cached_progress:
                job_progress[job_id] = cached_progress
            else:
                job_progress[job_id] = {
                    'progress': 0.0,
                    'stage': 'initializing',
                    'last_update': time.time(),
                    'estimated': False
                }
        
        current_progress = job_progress[job_id]['progress']
        current_stage = job_progress[job_id]['stage']
        
        # é¦–å…ˆå°è¯•ä»è¾“å‡ºä¸­è§£æç²¾ç¡®è¿›åº¦ï¼ˆä¸»è¦æ˜¯åŒ¹é… "æ•°å­—%|" æ ¼å¼ï¼‰
        parsed_progress = parse_progress_from_output(output_line)
        if parsed_progress is not None:
            job_progress[job_id].update({
                'progress': parsed_progress,
                'last_update': time.time(),
                'estimated': False
            })
            # å¦‚æœæœ‰ç²¾ç¡®è¿›åº¦ï¼Œä¹Ÿå°è¯•æ›´æ–°é˜¶æ®µä¿¡æ¯
            stage_info = estimate_progress_from_stage(output_line, parsed_progress)
            if stage_info:
                job_progress[job_id]['stage'] = stage_info['stage']
            # ç¼“å­˜æ›´æ–°çš„è¿›åº¦
            cache_job_progress(job_id)
            return
        
        # å¦‚æœæ²¡æœ‰ç²¾ç¡®è¿›åº¦ï¼Œæ ¹æ®é˜¶æ®µä¼°ç®—
        stage_info = estimate_progress_from_stage(output_line, current_progress)
        if stage_info:
            job_progress[job_id].update({
                'progress': stage_info['progress'],
                'stage': stage_info['stage'],
                'last_update': time.time(),
                'estimated': stage_info['estimated']
            })
            # ç¼“å­˜æ›´æ–°çš„è¿›åº¦
            cache_job_progress(job_id)
            return
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œæ ¹æ®æ—¶é—´ç¼“æ…¢å¢åŠ è¿›åº¦
        elapsed = time.time() - job_progress[job_id]['last_update']
        
        # æ›´ç§¯æçš„æ—¶é—´ä¼°ç®—ç­–ç•¥
        if elapsed > 5:  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
            # æ ¹æ®ä»»åŠ¡è¿è¡Œæ€»æ—¶é—´ä¼°ç®—è¿›åº¦
            total_elapsed = time.time() - job_metadata.get(job_id, {}).get('start_time', time.time())
            
            # åŸºäºç»éªŒçš„æ—¶é—´ä¼°ç®—ï¼ˆå‡è®¾ä¸€èˆ¬ä»»åŠ¡éœ€è¦2-5åˆ†é’Ÿï¼‰
            estimated_total_time = 180  # 3åˆ†é’Ÿçš„ä¼°ç®—
            time_based_progress = min(90.0, (total_elapsed / estimated_total_time) * 100)
            
            # æ ¹æ®å½“å‰é˜¶æ®µå†³å®šå¢é•¿é€Ÿåº¦
            if current_stage in ['generating_map', 'generating_timing', 'generating_kiai', 'inference', 'generating']:
                # ç”Ÿæˆé˜¶æ®µè¿›åº¦è¾ƒæ…¢ï¼Œæ¯æ¬¡å¢åŠ å°å¹…åº¦
                increment = min(2.0, (100 - current_progress) * 0.08)
            elif current_stage in ['loading', 'initializing']:
                # åŠ è½½é˜¶æ®µç›¸å¯¹è¾ƒå¿«
                increment = min(5.0, (30 - current_progress) * 0.2)
            else:
                # å…¶ä»–é˜¶æ®µè¿›åº¦ä¸­ç­‰
                increment = min(3.0, (100 - current_progress) * 0.1)
            
            # ä½¿ç”¨æ—¶é—´ä¼°ç®—å’Œå¢é‡çš„è¾ƒå¤§å€¼ï¼Œä½†ä¸è¶…è¿‡æ—¶é—´ä¼°ç®—çš„è¿›åº¦
            new_progress = min(
                time_based_progress,
                current_progress + increment,
                95.0  # æœ€å¤šåˆ°95%ï¼Œç•™ç»™å®é™…å®Œæˆæ£€æµ‹
            )
            
            if new_progress > current_progress:
                job_progress[job_id].update({
                    'progress': new_progress,
                    'last_update': time.time(),
                    'estimated': True
                })
                # ç¼“å­˜æ›´æ–°çš„è¿›åº¦
                cache_job_progress(job_id)

def parse_optional_int(value: str) -> Optional[int]:
    """è§£æå¯é€‰æ•´æ•°å‚æ•°"""
    if not value or value.strip() == "":
        return None
    try:
        return int(value)
    except ValueError:
        return None

def parse_optional_float(value: str) -> Optional[float]:
    """è§£æå¯é€‰æµ®ç‚¹æ•°å‚æ•°"""
    if not value or value.strip() == "":
        return None
    try:
        return float(value)
    except ValueError:
        return None

def save_audio_file(file: UploadFile, job_id: str) -> str:
    """ä¿å­˜éŸ³é¢‘æ–‡ä»¶åˆ°å›ºå®šç›®å½•"""
    # éªŒè¯æ–‡ä»¶ç±»å‹
    valid_extensions = {'.mp3', '.wav', '.ogg', '.m4a', '.flac'}
    if not file.filename:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰æä¾›æ–‡ä»¶")
    
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in valid_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(valid_extensions)}"
        )
    
    # ä½¿ç”¨job_idä½œä¸ºæ–‡ä»¶å
    audio_filename = f"{job_id}{file_ext}"
    audio_path = AUDIO_STORAGE / audio_filename
    
    return str(audio_path.absolute())

def build_command(job_id: str, audio_path: str, params: dict) -> List[str]:
    """æ„å»ºæ¨ç†å‘½ä»¤"""
    python_executable = sys.executable
    
    # åˆ›å»ºjobä¸“ç”¨è¾“å‡ºç›®å½•
    job_output_dir = OUTPUTS / job_id
    job_output_dir.mkdir(exist_ok=True)
    
    cmd = [python_executable, "inference.py", "-cn"]
    
    # æ¨¡å‹é…ç½®åç§°ï¼ˆå¯¹åº”configs/inference/ä¸‹çš„yamlæ–‡ä»¶ï¼‰
    config_name = params.get("model", "v30")  # é»˜è®¤ä½¿ç”¨v30é…ç½®
    cmd.append(config_name)
    
    # Hydraå‚æ•°å¼•ç”¨å‡½æ•°
    def hydra_quote(value):
        return f"'{str(value).replace(chr(39), chr(92) + chr(39))}'"
    
    def add_param(key, value):
        if value is not None and value != '':
            if key in {"audio_path", "output_path", "beatmap_path"}:
                cmd.append(f"{key}={hydra_quote(value)}")
            else:
                cmd.append(f"{key}={value}")
    
    def add_list_param(key, items):
        if items:
            quoted_items = [f"'{str(item)}'" for item in items]
            items_str = ",".join(quoted_items)
            cmd.append(f"{key}=[{items_str}]")
    
    # å¿…éœ€å‚æ•°
    add_param("audio_path", audio_path)
    add_param("output_path", str(job_output_dir))
    
    # å¯é€‰å‚æ•°
    add_param("gamemode", params.get("gamemode", 0))
    add_param("difficulty", params.get("difficulty"))
    add_param("year", params.get("year"))
    add_param("mapper_id", params.get("mapper_id"))
    
    # éš¾åº¦è®¾ç½®
    for param in ['hp_drain_rate', 'circle_size', 'overall_difficulty', 
                  'approach_rate', 'slider_multiplier', 'slider_tick_rate']:
        add_param(param, params.get(param))
    
    # Maniaä¸“ç”¨
    add_param("keycount", params.get("keycount"))
    add_param("hold_note_ratio", params.get("hold_note_ratio"))
    add_param("scroll_speed_ratio", params.get("scroll_speed_ratio"))
    
    # ç”Ÿæˆè®¾ç½®
    add_param("cfg_scale", params.get("cfg_scale", 1.0))
    add_param("temperature", params.get("temperature", 1.0))
    add_param("top_p", params.get("top_p", 0.95))
    add_param("seed", params.get("seed"))
    
    # æ—¶é—´è®¾ç½®
    add_param("start_time", params.get("start_time"))
    add_param("end_time", params.get("end_time"))
    
    # å¸ƒå°”é€‰é¡¹
    cmd.append(f"export_osz={str(params.get('export_osz', True)).lower()}")
    cmd.append(f"add_to_beatmap={str(params.get('add_to_beatmap', False)).lower()}")
    cmd.append(f"hitsounded={str(params.get('hitsounded', False)).lower()}")
    cmd.append(f"super_timing={str(params.get('super_timing', False)).lower()}")
    
    # åˆ—è¡¨å‚æ•°
    add_list_param("descriptors", params.get("descriptors"))
    add_list_param("negative_descriptors", params.get("negative_descriptors"))
    
    return cmd

def find_output_files(job_id: str) -> List[str]:
    """æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
    # å…ˆå°è¯•ä»ç¼“å­˜è·å–
    cached_files = get_cached_output_files(job_id)
    
    job_output_dir = OUTPUTS / job_id
    if not job_output_dir.exists():
        return cached_files or []
    
    files = []
    for file_path in job_output_dir.iterdir():
        if file_path.is_file():
            files.append(file_path.name)
    
    # ç¼“å­˜æ–‡ä»¶åˆ—è¡¨
    if files:
        cache_output_files(job_id, files)
    
    # å¦‚æœç›®å½•ä¸ºç©ºä½†ç¼“å­˜æœ‰æ•°æ®ï¼Œè¿”å›ç¼“å­˜æ•°æ®
    return files if files else (cached_files or [])

@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹"""
    return {
        "message": "Mapperatorinator API v2.0",
        "description": "ä¸Šä¼ éŸ³é¢‘+å‚æ•°ï¼Œç”Ÿæˆosu! beatmap",
        "endpoints": {
            "process": "POST /process - ä¸Šä¼ éŸ³é¢‘å’Œå‚æ•°å¼€å§‹å¤„ç†",
            "status": "GET /jobs/{job_id}/status - æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€",
            "progress": "GET /jobs/{job_id}/progress - æŸ¥è¯¢ä»»åŠ¡è¿›åº¦",
            "stream": "GET /jobs/{job_id}/stream - å®æ—¶è¾“å‡ºæµ",
            "download": "GET /jobs/{job_id}/download - ä¸‹è½½ç»“æœæ–‡ä»¶",
            "files": "GET /jobs/{job_id}/files - åˆ—å‡ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶",
            "cancel": "POST /jobs/{job_id}/cancel - å–æ¶ˆä»»åŠ¡"
        }
    }

@app.post("/process", response_model=ProcessResponse)
async def process_audio(
    audio_file: UploadFile = File(..., description="éŸ³é¢‘æ–‡ä»¶"),
    model: str = Form(default="v30", description="æ¨¡å‹é…ç½®åç§° (v30, v31, defaultç­‰)"),
    gamemode: int = Form(default=0, description="æ¸¸æˆæ¨¡å¼ (0=osu!, 1=taiko, 2=catch, 3=mania)"),
    difficulty: Optional[float] = Form(default=5.0, description="ç›®æ ‡éš¾åº¦æ˜Ÿçº§"),
    year: Optional[int] = Form(default=2023, description="å¹´ä»½"),
    mapper_id: Optional[str] = Form(default="", description="Mapper ID"),
    hp_drain_rate: Optional[float] = Form(default=5.0, description="HPæ¶ˆè€—ç‡"),
    circle_size: Optional[float] = Form(default=4.0, description="åœ†åœˆå¤§å°"),
    overall_difficulty: Optional[float] = Form(default=8.0, description="æ•´ä½“éš¾åº¦"),
    approach_rate: Optional[float] = Form(default=9.0, description="æ¥è¿‘é€Ÿåº¦"),
    slider_multiplier: Optional[float] = Form(default=1.4, description="æ»‘æ¡å€ç‡"),
    slider_tick_rate: Optional[float] = Form(default=1.0, description="æ»‘æ¡tickç‡"),
    keycount: Optional[str] = Form(default="", description="æŒ‰é”®æ•°é‡(mania)"),
    hold_note_ratio: Optional[str] = Form(default="", description="é•¿æŒ‰éŸ³ç¬¦æ¯”ä¾‹(mania)"),
    scroll_speed_ratio: Optional[str] = Form(default="", description="æ»šåŠ¨é€Ÿåº¦æ¯”ä¾‹"),
    cfg_scale: float = Form(default=1.0, description="CFGå¼•å¯¼å¼ºåº¦"),
    temperature: float = Form(default=0.9, description="é‡‡æ ·æ¸©åº¦"),
    top_p: float = Form(default=0.9, description="Top-pé‡‡æ ·"),
    seed: Optional[str] = Form(default="", description="éšæœºç§å­"),
    start_time: Optional[str] = Form(default="", description="å¼€å§‹æ—¶é—´(æ¯«ç§’)"),
    end_time: Optional[str] = Form(default="", description="ç»“æŸæ—¶é—´(æ¯«ç§’)"),
    export_osz: bool = Form(default=True, description="å¯¼å‡º.oszæ–‡ä»¶"),
    add_to_beatmap: bool = Form(default=False, description="æ·»åŠ åˆ°ç°æœ‰beatmap"),
    hitsounded: bool = Form(default=False, description="åŒ…å«æ‰“å‡»éŸ³æ•ˆ"),
    super_timing: bool = Form(default=False, description="ä½¿ç”¨è¶…çº§æ—¶é—´ç”Ÿæˆ"),
    descriptors: Optional[str] = Form(None, description="é£æ ¼æè¿°ç¬¦(JSONæ•°ç»„)"),
    negative_descriptors: Optional[str] = Form(None, description="è´Ÿé¢æè¿°ç¬¦(JSONæ•°ç»„)")
):
    """å¤„ç†éŸ³é¢‘æ–‡ä»¶å’Œå‚æ•°"""
    job_id = str(uuid.uuid4())
    
    with process_lock:
        if job_id in active_processes:
            raise HTTPException(status_code=409, detail="ä»»åŠ¡IDå†²çª")
        
        try:
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            audio_path = save_audio_file(audio_file, job_id)
            
            # ä¿å­˜å®é™…çš„éŸ³é¢‘å†…å®¹
            with open(audio_path, "wb") as buffer:
                content = await audio_file.read()
                buffer.write(content)
            
            # è§£æJSONå‚æ•°
            desc_list = None
            if descriptors and descriptors.strip():
                try:
                    desc_list = json.loads(descriptors)
                except json.JSONDecodeError:
                    desc_list = None
            
            neg_desc_list = None
            if negative_descriptors and negative_descriptors.strip():
                try:
                    neg_desc_list = json.loads(negative_descriptors)
                except json.JSONDecodeError:
                    neg_desc_list = None
            
            # æ„å»ºå‚æ•°å­—å…¸ï¼Œå¤„ç†å­—ç¬¦ä¸²å‚æ•°è½¬æ¢
            params = {
                "model": model,
                "gamemode": gamemode,
                "difficulty": difficulty,
                "year": year,
                "mapper_id": parse_optional_int(mapper_id) if mapper_id else None,
                "hp_drain_rate": hp_drain_rate,
                "circle_size": circle_size,
                "overall_difficulty": overall_difficulty,
                "approach_rate": approach_rate,
                "slider_multiplier": slider_multiplier,
                "slider_tick_rate": slider_tick_rate,
                "keycount": parse_optional_int(keycount) if keycount else None,
                "hold_note_ratio": parse_optional_float(hold_note_ratio) if hold_note_ratio else None,
                "scroll_speed_ratio": parse_optional_float(scroll_speed_ratio) if scroll_speed_ratio else None,
                "cfg_scale": cfg_scale,
                "temperature": temperature,
                "top_p": top_p,
                "seed": parse_optional_int(seed) if seed else None,
                "start_time": parse_optional_int(start_time) if start_time else None,
                "end_time": parse_optional_int(end_time) if end_time else None,
                "export_osz": export_osz,
                "add_to_beatmap": add_to_beatmap,
                "hitsounded": hitsounded,
                "super_timing": super_timing,
                "descriptors": desc_list,
                "negative_descriptors": neg_desc_list
            }
            
            # æ„å»ºå‘½ä»¤
            cmd = build_command(job_id, audio_path, params)
            print(f"å¯åŠ¨ä»»åŠ¡ {job_id}: {' '.join(cmd)}")
            
            # å¯åŠ¨è¿›ç¨‹
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                bufsize=1,
                universal_newlines=True,
                encoding='utf-8',
                errors='replace'
            )
            
            active_processes[job_id] = process
            process_outputs[job_id] = []
            job_metadata[job_id] = {
                "audio_path": audio_path,
                "audio_filename": audio_file.filename,
                "params": params,
                "start_time": time.time()
            }
            job_progress[job_id] = {
                "progress": 0.0,
                "stage": "started",
                "last_update": time.time(),
                "estimated": False
            }
            
            # ç¼“å­˜åˆå§‹ä»»åŠ¡ä¿¡æ¯
            cache_job_metadata(job_id)
            cache_job_progress(job_id)
            
            # å¯åŠ¨åå°çº¿ç¨‹ç›‘æ§è¿›ç¨‹è¾“å‡º
            def monitor_process_output(job_id, process):
                """åå°ç›‘æ§è¿›ç¨‹è¾“å‡º"""
                try:
                    if process.stdout:
                        for line in iter(process.stdout.readline, ""):
                            if not line:
                                break
                            
                            # æ›´æ–°è¿›åº¦
                            update_job_progress(job_id, line)
                            
                            # å­˜å‚¨è¾“å‡º
                            with process_lock:
                                if job_id in process_outputs:
                                    process_outputs[job_id].append(line)
                    
                    # è¿›ç¨‹ç»“æŸåæ ‡è®°è¿›åº¦ä¸ºå®Œæˆ
                    return_code = process.wait()
                    with process_lock:
                        if job_id in job_progress:
                            if return_code == 0:
                                job_progress[job_id]['progress'] = 100.0
                                job_progress[job_id]['stage'] = 'completed'
                            else:
                                job_progress[job_id]['stage'] = 'failed'
                            job_progress[job_id]['completed_at'] = time.time()
                            # ç¼“å­˜æœ€ç»ˆè¿›åº¦çŠ¶æ€
                            cache_job_progress(job_id)
                
                except Exception as e:
                    print(f"ç›‘æ§è¿›ç¨‹è¾“å‡ºé”™è¯¯ {job_id}: {e}")
                    with process_lock:
                        if job_id in job_progress:
                            job_progress[job_id]['stage'] = 'error'
                            cache_job_progress(job_id)
            
            # å¯åŠ¨ç›‘æ§çº¿ç¨‹
            monitor_thread = threading.Thread(
                target=monitor_process_output, 
                args=(job_id, process),
                daemon=True
            )
            monitor_thread.start()
            
            print(f"ä»»åŠ¡ {job_id} å·²å¯åŠ¨ (PID: {process.pid})")
            
            return ProcessResponse(
                job_id=job_id,
                status="started",
                message=f"å¤„ç†å·²å¼€å§‹ï¼ŒéŸ³é¢‘æ–‡ä»¶: {audio_file.filename}"
            )
            
        except json.JSONDecodeError as e:
            raise HTTPException(status_code=400, detail=f"JSONå‚æ•°è§£æé”™è¯¯: {str(e)}")
        except Exception as e:
            print(f"å¯åŠ¨ä»»åŠ¡å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"å¯åŠ¨å¤„ç†å¤±è´¥: {str(e)}")

@app.get("/jobs/{job_id}/status", response_model=JobStatus)
async def get_status(job_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
    with process_lock:
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨ï¼ˆåŒ…æ‹¬å·²å®Œæˆçš„ä»»åŠ¡ï¼‰
        if job_id not in active_processes and job_id not in job_progress:
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            cached_progress = get_cached_job_progress(job_id)
            cached_metadata = get_cached_job_metadata(job_id)
            
            if not cached_progress and not cached_metadata:
                raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
            
            # ä»ç¼“å­˜æ¢å¤æ•°æ®
            if cached_progress:
                job_progress[job_id] = cached_progress
            if cached_metadata:
                job_metadata[job_id] = cached_metadata
        
        metadata = job_metadata.get(job_id, {})
        progress_info = job_progress.get(job_id, {})
        current_progress = progress_info.get('progress', 0.0)
        stage = progress_info.get('stage', 'unknown')
        
        # å¦‚æœä»»åŠ¡è¿˜åœ¨æ´»åŠ¨è¿›ç¨‹ä¸­
        if job_id in active_processes:
            process = active_processes[job_id]
            return_code = process.poll()
            
            if return_code is None:
                # è¿›ç¨‹è¿è¡Œä¸­
                return JobStatus(
                    job_id=job_id,
                    status="running",
                    message=f"æ­£åœ¨å¤„ç†ä¸­... ({stage})",
                    progress=current_progress,
                    output_files=None,
                    error=None
                )
            elif return_code == 0:
                # è¿›ç¨‹æˆåŠŸå®Œæˆ
                output_files = find_output_files(job_id)
                # ç¡®ä¿è¿›åº¦ä¸º100%
                with process_lock:
                    if job_id in job_progress:
                        job_progress[job_id]['progress'] = 100.0
                        cache_job_progress(job_id)
                
                return JobStatus(
                    job_id=job_id,
                    status="completed",
                    message="å¤„ç†å®Œæˆ",
                    progress=100.0,
                    output_files=output_files,
                    error=None
                )
            else:
                # è¿›ç¨‹å¤±è´¥
                return JobStatus(
                    job_id=job_id,
                    status="failed",
                    message="å¤„ç†å¤±è´¥",
                    progress=current_progress,
                    output_files=None,
                    error=f"è¿›ç¨‹é€€å‡ºä»£ç : {return_code}"
                )
        else:
            # ä»»åŠ¡å·²ä»æ´»åŠ¨è¿›ç¨‹ä¸­ç§»é™¤ï¼Œæ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
            output_files = find_output_files(job_id)
            if output_files:
                # æœ‰è¾“å‡ºæ–‡ä»¶ï¼Œè¯´æ˜æˆåŠŸå®Œæˆ
                return JobStatus(
                    job_id=job_id,
                    status="completed",
                    message="å¤„ç†å®Œæˆ",
                    progress=100.0,
                    output_files=output_files,
                    error=None
                )
            else:
                # æ²¡æœ‰è¾“å‡ºæ–‡ä»¶ï¼Œå¯èƒ½å¤±è´¥æˆ–æœªçŸ¥çŠ¶æ€
                final_progress = 100.0 if current_progress >= 100.0 else current_progress
                status = "completed" if final_progress >= 100.0 else "failed"
                
                return JobStatus(
                    job_id=job_id,
                    status=status,
                    message="å¤„ç†å®Œæˆ" if status == "completed" else "å¤„ç†å¯èƒ½å¤±è´¥",
                    progress=final_progress,
                    output_files=output_files if output_files else None,
                    error=None if status == "completed" else "æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶"
                )

@app.get("/jobs/{job_id}/progress", response_model=ProgressResponse)
async def get_progress(job_id: str):
    """è·å–ä»»åŠ¡è¯¦ç»†è¿›åº¦ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
    with process_lock:
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        if job_id not in active_processes and job_id not in job_progress:
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            cached_progress = get_cached_job_progress(job_id)
            if not cached_progress:
                raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
            
            # ä»ç¼“å­˜æ¢å¤è¿›åº¦æ•°æ®
            job_progress[job_id] = cached_progress
        
        progress_info = job_progress.get(job_id, {})
        
        # ç¡®å®šä»»åŠ¡çŠ¶æ€
        if job_id in active_processes:
            process = active_processes[job_id]
            return_code = process.poll()
            
            if return_code is None:
                status = "running"
            elif return_code == 0:
                status = "completed"
            else:
                status = "failed"
        else:
            # ä»»åŠ¡å·²å®Œæˆæˆ–å¤±è´¥
            status = "completed" if progress_info.get('progress', 0) == 100.0 else "unknown"
        
        return ProgressResponse(
            job_id=job_id,
            progress=progress_info.get('progress', 0.0),
            stage=progress_info.get('stage', 'unknown'),
            estimated=progress_info.get('estimated', True),
            last_update=progress_info.get('last_update', time.time()),
            status=status
        )

@app.get("/jobs/{job_id}/stream")
async def stream_output(job_id: str):
    """å®æ—¶è¾“å‡ºæµ"""
    
    async def event_generator():
        with process_lock:
            if job_id not in active_processes:
                yield {
                    "event": "error",
                    "data": "ä»»åŠ¡ä¸å­˜åœ¨"
                }
                return
            
            process = active_processes[job_id]
        
        print(f"å¼€å§‹æµå¼è¾“å‡ºä»»åŠ¡ {job_id}")
        
        try:
            if process.stdout:
                for line in iter(process.stdout.readline, ""):
                    if not line:
                        break
                    
                    # æ›´æ–°è¿›åº¦
                    update_job_progress(job_id, line)
                    
                    # å­˜å‚¨è¾“å‡º
                    with process_lock:
                        if job_id in process_outputs:
                            process_outputs[job_id].append(line)
                    
                    # è·å–å½“å‰è¿›åº¦ä¿¡æ¯
                    progress_info = job_progress.get(job_id, {})
                    progress_value = progress_info.get('progress', 0.0)
                    
                    yield {
                        "event": "output",
                        "data": line.rstrip(),
                        "progress": progress_value
                    }
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            return_code = process.wait()
            
            # æ ‡è®°è¿›åº¦ä¸ºå®Œæˆ
            with process_lock:
                if job_id in job_progress:
                    job_progress[job_id]['progress'] = 100.0
            
            if return_code == 0:
                yield {
                    "event": "completed",
                    "data": "å¤„ç†å®Œæˆ",
                    "progress": 100.0
                }
            else:
                yield {
                    "event": "failed",
                    "data": f"å¤„ç†å¤±è´¥ï¼Œé€€å‡ºä»£ç : {return_code}",
                    "progress": job_progress.get(job_id, {}).get('progress', 0.0)
                }
                
        except Exception as e:
            print(f"æµå¼è¾“å‡ºé”™è¯¯ {job_id}: {e}")
            yield {
                "event": "error",
                "data": f"æµå¼è¾“å‡ºé”™è¯¯: {str(e)}"
            }
        finally:
            # æ¸…ç†
            with process_lock:
                if job_id in active_processes:
                    del active_processes[job_id]
                if job_id in job_progress:
                    # ä¿ç•™è¿›åº¦ä¿¡æ¯ä¸€æ®µæ—¶é—´ï¼Œæ–¹ä¾¿æŸ¥è¯¢
                    job_progress[job_id]['completed_at'] = time.time()
                print(f"æ¸…ç†ä»»åŠ¡ {job_id}")
    
    return EventSourceResponse(event_generator())

@app.get("/jobs/{job_id}/download")
async def download_result(job_id: str, filename: Optional[str] = None):
    """ä¸‹è½½ç»“æœæ–‡ä»¶"""
    job_output_dir = OUTPUTS / job_id
    
    if not job_output_dir.exists():
        raise HTTPException(status_code=404, detail="ä»»åŠ¡è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
    
    # æŸ¥æ‰¾æ–‡ä»¶
    output_files = find_output_files(job_id)
    if not output_files:
        raise HTTPException(status_code=404, detail="æ²¡æœ‰æ‰¾åˆ°è¾“å‡ºæ–‡ä»¶")
    
    # ç¡®å®šè¦ä¸‹è½½çš„æ–‡ä»¶
    if filename:
        if filename not in output_files:
            raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
        target_file = filename
    else:
        # ä¼˜å…ˆé€‰æ‹©.oszæ–‡ä»¶
        osz_files = [f for f in output_files if f.endswith('.osz')]
        if osz_files:
            target_file = osz_files[0]
        else:
            target_file = output_files[0]
    
    file_path = job_output_dir / target_file
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    return FileResponse(
        path=str(file_path),
        filename=target_file,
        media_type='application/octet-stream'
    )

@app.get("/jobs/{job_id}/files")
async def list_files(job_id: str):
    """åˆ—å‡ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
    job_output_dir = OUTPUTS / job_id
    
    if not job_output_dir.exists():
        return {"files": []}
    
    files = []
    for file_path in job_output_dir.iterdir():
        if file_path.is_file():
            files.append({
                "name": file_path.name,
                "size": file_path.stat().st_size,
                "type": file_path.suffix,
                "download_url": f"/jobs/{job_id}/download?filename={file_path.name}"
            })
    
    return {"files": files}

@app.post("/jobs/{job_id}/cancel")
async def cancel_job(job_id: str):
    """å–æ¶ˆä»»åŠ¡"""
    with process_lock:
        if job_id not in active_processes:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        process = active_processes[job_id]
        
        if process.poll() is not None:
            return {"status": "already_finished", "message": "ä»»åŠ¡å·²å®Œæˆ"}
        
        try:
            process.terminate()
            
            # ç­‰å¾…ä¼˜é›…ç»ˆæ­¢
            try:
                process.wait(timeout=5)
                message = "ä»»åŠ¡å·²å–æ¶ˆ"
            except subprocess.TimeoutExpired:
                process.kill()
                message = "ä»»åŠ¡å·²å¼ºåˆ¶ç»ˆæ­¢"
            
            del active_processes[job_id]
            
            return {
                "status": "cancelled",
                "message": message
            }
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {str(e)}")

@app.get("/jobs")
async def list_jobs():
    """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡"""
    with process_lock:
        jobs = []
        for job_id, process in active_processes.items():
            return_code = process.poll()
            status = "completed" if return_code == 0 else "failed" if return_code is not None else "running"
            
            metadata = job_metadata.get(job_id, {})
            
            jobs.append({
                "job_id": job_id,
                "status": status,
                "audio_filename": metadata.get("audio_filename"),
                "start_time": metadata.get("start_time"),
                "pid": process.pid
            })
        
        return {"jobs": jobs}

def cleanup_finished_jobs():
    """æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡"""
    with process_lock:
        current_time = time.time()
        
        # æ¸…ç†å·²å®Œæˆçš„è¿›ç¨‹
        finished_jobs = []
        for job_id, process in active_processes.items():
            if process.poll() is not None:
                finished_jobs.append(job_id)
        
        for job_id in finished_jobs:
            print(f"æ¸…ç†å·²å®Œæˆä»»åŠ¡ {job_id}")
            del active_processes[job_id]
        
        # æ¸…ç†è¶…è¿‡1å°æ—¶çš„è¿›åº¦ä¿¡æ¯
        old_progress_jobs = []
        for job_id, progress_info in job_progress.items():
            completed_at = progress_info.get('completed_at')
            if completed_at and (current_time - completed_at) > 3600:  # 1å°æ—¶
                old_progress_jobs.append(job_id)
        
        for job_id in old_progress_jobs:
            print(f"æ¸…ç†æ—§è¿›åº¦ä¿¡æ¯ {job_id}")
            del job_progress[job_id]
            # æ¸…ç†Redisç¼“å­˜
            cache_delete(f"job_progress:{job_id}")
            cache_delete(f"job_metadata:{job_id}")
            cache_delete(f"output_files:{job_id}")

def cleanup_redis_cache():
    """æ¸…ç†è¿‡æœŸçš„Redisç¼“å­˜"""
    if not redis_client:
        return
    
    try:
        # è·å–æ‰€æœ‰jobç›¸å…³çš„é”®
        job_keys = []
        for pattern in ["job_progress:*", "job_metadata:*", "output_files:*"]:
            keys = redis_client.keys(pattern)
            if keys and isinstance(keys, (list, tuple)):
                job_keys.extend(keys)
        
        # æ£€æŸ¥å¹¶åˆ é™¤è¶…è¿‡24å°æ—¶çš„ç¼“å­˜
        current_time = time.time()
        for key in job_keys:
            try:
                ttl = redis_client.ttl(key)
                # å¦‚æœé”®æ²¡æœ‰è¿‡æœŸæ—¶é—´æˆ–è€…å·²ç»è¿‡æœŸå¾ˆä¹…ï¼Œåˆ é™¤å®ƒ
                if isinstance(ttl, int) and (ttl == -1 or ttl < -86400):  # è¶…è¿‡24å°æ—¶
                    redis_client.delete(key)
                    print(f"åˆ é™¤è¿‡æœŸç¼“å­˜é”®: {key}")
            except redis.exceptions.RedisError:
                continue
                
    except redis.exceptions.RedisError as e:
        print(f"æ¸…ç†Redisç¼“å­˜å¤±è´¥: {e}")

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨äº‹ä»¶"""
    print("ğŸš€ å¯åŠ¨ Mapperatorinator API v2.0...")
    print(f"ğŸ“ éŸ³é¢‘å­˜å‚¨ç›®å½•: {AUDIO_STORAGE.absolute()}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {OUTPUTS.absolute()}")
    
    if redis_client:
        print("âœ… Redisç¼“å­˜å·²å¯ç”¨ (db=1)")
    else:
        print("âš ï¸ Redisç¼“å­˜æœªå¯ç”¨ï¼Œä½¿ç”¨å†…å­˜ç¼“å­˜")
    
    # å¯åŠ¨åå°æ¸…ç†ä»»åŠ¡
    async def periodic_cleanup():
        while True:
            await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
            cleanup_finished_jobs()
            
            # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡Redisç¼“å­˜
            import time
            if int(time.time()) % 3600 < 300:  # åœ¨æ•´ç‚¹å5åˆ†é’Ÿå†…æ‰§è¡Œ
                cleanup_redis_cache()
    
    asyncio.create_task(periodic_cleanup())

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­äº‹ä»¶"""
    print("ğŸ›‘ å…³é—­ Mapperatorinator API...")
    
    # ç»ˆæ­¢æ‰€æœ‰æ´»åŠ¨è¿›ç¨‹
    with process_lock:
        for job_id, process in active_processes.items():
            if process.poll() is None:
                print(f"ç»ˆæ­¢ä»»åŠ¡ {job_id}")
                process.terminate()

@app.get("/debug/redis")
async def redis_status():
    """RedisçŠ¶æ€å’Œç¼“å­˜ä¿¡æ¯"""
    if not redis_client:
        return {
            "status": "disabled",
            "message": "Redisæœªå¯ç”¨",
            "cache_stats": None
        }
    
    try:
        # æµ‹è¯•è¿æ¥
        redis_client.ping()
        
        # è·å–Redisä¿¡æ¯
        info = redis_client.info()
        redis_info = {}
        if isinstance(info, dict):
            redis_info = {
                "version": info.get("redis_version"),
                "used_memory": info.get("used_memory_human"),
                "connected_clients": info.get("connected_clients"),
                "total_commands_processed": info.get("total_commands_processed")
            }
        
        # è·å–ç¼“å­˜é”®ç»Ÿè®¡
        cache_stats = {}
        for prefix in ["job_progress", "job_metadata", "output_files", "model_config"]:
            pattern = f"{prefix}:*"
            keys = redis_client.keys(pattern)
            if isinstance(keys, (list, tuple)):
                cache_stats[prefix] = len(keys)
            else:
                cache_stats[prefix] = 0
        
        return {
            "status": "connected",
            "database": 1,
            "redis_info": redis_info,
            "cache_stats": cache_stats
        }
        
    except redis.exceptions.RedisError as e:
        return {
            "status": "error",
            "message": str(e),
            "cache_stats": None
        }

@app.get("/jobs/{job_id}/debug")
async def debug_job_output(job_id: str):
    """è°ƒè¯•ç«¯ç‚¹ï¼šæŸ¥çœ‹ä»»åŠ¡çš„æœ€è¿‘è¾“å‡ºè¡Œå’Œç¼“å­˜çŠ¶æ€"""
    with process_lock:
        if job_id not in active_processes and job_id not in process_outputs:
            # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰æ•°æ®
            cached_progress = get_cached_job_progress(job_id)
            cached_metadata = get_cached_job_metadata(job_id)
            if not cached_progress and not cached_metadata:
                raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        # è·å–æœ€è¿‘çš„è¾“å‡ºè¡Œ
        recent_outputs = process_outputs.get(job_id, [])[-20:]  # æœ€è¿‘20è¡Œ
        progress_info = job_progress.get(job_id, {})
        metadata = job_metadata.get(job_id, {})
        
        # è·å–ç¼“å­˜çŠ¶æ€
        cache_status = {}
        if redis_client:
            cache_status = {
                "progress_cached": cache_exists(f"job_progress:{job_id}"),
                "metadata_cached": cache_exists(f"job_metadata:{job_id}"),
                "files_cached": cache_exists(f"output_files:{job_id}")
            }
        
        return {
            "job_id": job_id,
            "recent_outputs": recent_outputs,
            "progress_info": progress_info,
            "total_output_lines": len(process_outputs.get(job_id, [])),
            "start_time": metadata.get("start_time"),
            "elapsed_time": time.time() - metadata.get("start_time", time.time()),
            "is_active": job_id in active_processes,
            "cache_status": cache_status
        }

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Mapperatorinator API Server v2.0")
    parser.add_argument("--host", default="127.0.0.1", help="ç»‘å®šä¸»æœº")
    parser.add_argument("--port", type=int, default=8000, help="ç»‘å®šç«¯å£")
    parser.add_argument("--reload", action="store_true", help="å¯ç”¨è‡ªåŠ¨é‡è½½")
    
    args = parser.parse_args()
    
    print("ğŸ® Mapperatorinator API v2.0")
    print("=" * 50)
    print(f"ğŸŒ APIæ–‡æ¡£: http://{args.host}:{args.port}/docs")
    print(f"ğŸ“š ReDoc: http://{args.host}:{args.port}/redoc")
    print("=" * 50)
    
    uvicorn.run(
        "api_v2:app",
        host=args.host,
        port=args.port,
        reload=args.reload,
        access_log=True
    )
